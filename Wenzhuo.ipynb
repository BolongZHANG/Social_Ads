{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from scipy import sparse\n",
    "import os\n",
    "from problem import get_train_data, get_test_data\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model to submit\n",
    "The submission consists of two files: feature_extractor.py which defines a FeatureExtractor class, and classifier.py which defines a Classifier class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two classes in the following only give you an easy example, it helps you to understand how the challenge runs, you need to improve them by selecting the most efficent features and models with proper parameters.\n",
    "\n",
    "We use FeatureExtrator to transform and select data so that they can be used directly for the training model. As there are some categorical features, we need to encode them firstly. In the following FeatureExtractor class, in order to accelerate the running time, we keep only one dimensionnal feature, and use OneHotEncoder which creates a binary column for each category and returns a sparse matrix or dense array (For multidimentional features, you can use the defined function Vectorize directly to get the array). \n",
    "\n",
    "Then we define Classifier( ) class to create the training model. We've provided an easy model LogisticRegression( ), you can modify the model in this class to have a better output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_df, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        return _transform(X_df)\n",
    "\n",
    "def _transform(X_df):\n",
    "    X_df_new = X_df.copy()\n",
    "    train, _ = get_train_data()\n",
    "    test, _ = get_test_data()\n",
    "    data_new = pd.concat([train, test])\n",
    "    \n",
    "    X_df_new = X_df_new.fillna('-1')  # replace missing values NaN\n",
    "    data_new = data_new.fillna('-1')\n",
    "\n",
    "    one_hot_feature = ['LBS', 'age', 'carrier', 'consumptionAbility',\n",
    "        'education','gender', 'house', 'os', 'ct', 'marriageStatus',\n",
    "        'advertiserId', 'campaignId', 'creativeId', 'adCategoryId',\n",
    "        'productId', 'productType']  # features with only one scalar\n",
    "        \n",
    "    vector_feature = ['appIdAction', 'appIdInstall', 'interest1',\n",
    "        'interest2', 'interest3', 'interest4', 'interest5', 'kw1',\n",
    "        'kw2', 'kw3', 'topic1', 'topic2', 'topic3']  # vector features\n",
    "\n",
    "    X_df_new = labelEncoder(data_new, X_df_new, one_hot_feature)\n",
    "    data_new = labelEncoder(data_new, data_new, one_hot_feature)  # normalize features\n",
    "\n",
    "    X_sparse = OneHot(data_new, X_df_new, one_hot_feature)\n",
    "    #X_sparse = Vectorize(data_new, X_df_new, vector_feature, X_sparse)\n",
    "\n",
    "    return X_sparse.tocsr()\n",
    "\n",
    "\n",
    "def labelEncoder(data, X_df, one_hot_feature):  # normalize features\n",
    "    le = LabelEncoder()\n",
    "    for feature in one_hot_feature:\n",
    "        try:\n",
    "            le.fit(data[feature].apply(int))\n",
    "            X_df[feature] = le.transform(X_df[feature].apply(int))\n",
    "\n",
    "        except:\n",
    "            le.fit(data[feature])\n",
    "            X_df[feature] = le.transform(X_df[feature])\n",
    "\n",
    "    return X_df\n",
    "\n",
    "\n",
    "def OneHot(data, X_df, one_hot_feature):\n",
    "    enc = OneHotEncoder(categories='auto')\n",
    "    X_sparse = X_df[['creativeSize']]\n",
    "    for feature in one_hot_feature:\n",
    "        enc.fit(data[feature].values.reshape(-1, 1))\n",
    "        X_onehot = enc.transform(X_df[feature].values.reshape(-1, 1))\n",
    "        X_sparse = sparse.hstack((X_sparse, X_onehot))\n",
    "    print('one hot finished')\n",
    "    return X_sparse\n",
    "\n",
    "\n",
    "def Vectorize(data, X_df, vector_feature, X_sparse):\n",
    "    cv = CountVectorizer()\n",
    "    for feature in vector_feature:\n",
    "        cv.fit(data[feature])\n",
    "        X_vec = cv.transform(X_df[feature])\n",
    "        X_sparse = sparse.hstack((X_sparse, X_vec))\n",
    "    print('cv finished')\n",
    "    return X_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.model = make_pipeline(StandardScaler(with_mean=False), LogisticRegression())\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_pipeline(FeatureExtractor(), Classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_test, y_pred[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sumbit on Ramp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting the challenge, you can test your model locally using the test data we've provided.\n",
    "\n",
    "Run the following cell to download ramp-workflow package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install ramp-workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modifiy the two files classifier.py and feature_extractor.py in the folder submissions/starting_kit, and run ramp_test_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ramp_test_submission --submission starting_kit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the score is good enough, you can submit your codes on the website ramp. Go to your sandbox and copy-paste the two files or upload them from local local files. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the FeatureExtractor, after encoding all features by the functions OneHot( ) and Vectorize( ), you can get a sparse matrix containing the complete informations for whole features. The dimension of features can be large because of multidimentional features.  Selection of important features for sparse matrix is needed in this case. RandomForest or Lightgbm are good choices for selection. \n",
    "\n",
    "Origin features after encoding are not good enough to train a great model, you can also add some statistical features such as the frequency of one uid appearing in training data. These statistical features may have a big relerance with labels.\n",
    "\n",
    "Choosing a good model is also important. You can try some more efficient model such as ffm, lightgbm, or xgboost, even using some model on deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
