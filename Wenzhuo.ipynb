{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import OneHotEncoder,LabelEncoder\n",
    "from scipy import sparse\n",
    "import os\n",
    "from problem import get_train_data, get_test_data\n",
    "from joblib import Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_train_data()\n",
    "X_test, y_test = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X_df, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X_df):\n",
    "        return _transform(X_df)\n",
    "\n",
    "def _transform(X_df):\n",
    "    X_df_new = X_df.copy()\n",
    "    train, _ = get_train_data()\n",
    "    test, _ = get_test_data()\n",
    "    data_new = pd.concat([train, test])\n",
    "    \n",
    "    X_df_new = X_df_new.fillna('-1')  # replace missing values NaN\n",
    "    data_new = data_new.fillna('-1')\n",
    "\n",
    "    one_hot_feature = ['LBS', 'age', 'carrier', 'consumptionAbility',\n",
    "        'education','gender', 'house', 'os', 'ct', 'marriageStatus',\n",
    "        'advertiserId', 'campaignId', 'creativeId', 'adCategoryId',\n",
    "        'productId', 'productType']  # features with only one scalar\n",
    "        \n",
    "    vector_feature = ['appIdAction', 'appIdInstall', 'interest1',\n",
    "        'interest2', 'interest3', 'interest4', 'interest5', 'kw1',\n",
    "        'kw2', 'kw3', 'topic1', 'topic2', 'topic3']  # vector features\n",
    "\n",
    "    X_df_new = labelEncoder(data_new, X_df_new, one_hot_feature)\n",
    "    data_new = labelEncoder(data_new, data_new, one_hot_feature)  # normalize features\n",
    "\n",
    "    X_sparse = OneHot(data_new, X_df_new, one_hot_feature)\n",
    "    X_sparse = Vectorize(data_new, X_df_new, vector_feature, X_sparse)\n",
    "\n",
    "    return X_sparse.tocsr()\n",
    "\n",
    "\n",
    "def labelEncoder(data, X_df, one_hot_feature):  # normalize features\n",
    "    le = LabelEncoder()\n",
    "    for feature in one_hot_feature:\n",
    "        try:\n",
    "            le.fit(data[feature].apply(int))\n",
    "            X_df[feature] = le.transform(X_df[feature].apply(int))\n",
    "\n",
    "        except:\n",
    "            le.fit(data[feature])\n",
    "            X_df[feature] = le.transform(X_df[feature])\n",
    "\n",
    "    return X_df\n",
    "\n",
    "\n",
    "def OneHot(data, X_df, one_hot_feature):\n",
    "    enc = OneHotEncoder(categories='auto')\n",
    "    X_sparse = X_df[['creativeSize']]\n",
    "    for feature in one_hot_feature:\n",
    "        enc.fit(data[feature].values.reshape(-1, 1))\n",
    "        X_onehot = enc.transform(X_df[feature].values.reshape(-1, 1))\n",
    "        X_sparse = sparse.hstack((X_sparse, X_onehot))\n",
    "    print('one hot finished')\n",
    "    return X_sparse\n",
    "\n",
    "\n",
    "def Vectorize(data, X_df, vector_feature, X_sparse):\n",
    "    cv = CountVectorizer()\n",
    "    for feature in vector_feature:\n",
    "        cv.fit(data[feature])\n",
    "        X_vec = cv.transform(X_df[feature])\n",
    "        X_sparse = sparse.hstack((X_sparse, X_vec))\n",
    "    print('cv finished')\n",
    "    return X_sparse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "class Classifier(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.model = lgb.LGBMClassifier(\n",
    "            boosting_type='gbdt', num_leaves=31, reg_alpha=0.0, reg_lambda=1,\n",
    "            max_depth=10, n_estimators=15, objective='binary',\n",
    "            subsample=0.7, colsample_bytree=0.7, subsample_freq=1,\n",
    "            learning_rate=0.05, min_child_weight=50, random_state=2018, \n",
    "            n_jobs=-1, class_weight='balanced')\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-wangsun-Documents-4A_DS-Big Data Camp-group project-Social_Ads-__ipython-input__._transform...\n",
      "_transform(        advertiserId  campaignId  creativeId  creativeSize  adCategoryId  \\\n",
      "0                915         994       27461            60            51   \n",
      "1                915         994       27461            60            51   \n",
      "2                915         994       27461            60            51   \n",
      "3                915         994       27461            60            51   \n",
      "4                915         994       27461            60            51   \n",
      "5                915         994       27461            60            51   \n",
      "6                915         994       27461            60            51   \n",
      "7                915         994       27461            60            51   \n",
      "8               ...)\n",
      "one hot finished\n",
      "cv finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangsun/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Persisting input arguments took 47.81s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "______________________________________________________transform - 273.4s, 4.6min\n",
      "________________________________________________________________________________\n",
      "[Memory] Calling __main__--home-wangsun-Documents-4A_DS-Big Data Camp-group project-Social_Ads-__ipython-input__._transform...\n",
      "_transform(        advertiserId  campaignId  creativeId  creativeSize  adCategoryId  \\\n",
      "0             133292      464828     1334609            22            74   \n",
      "1             133292      464828     1334609            22            74   \n",
      "2             133292      464828     1334609            22            74   \n",
      "3             133292      464828     1334609            22            74   \n",
      "4             133292      464828     1334609            22            74   \n",
      "5             133292      464828     1334609            22            74   \n",
      "6             133292      464828     1334609            22            74   \n",
      "7             133292      464828     1334609            22            74   \n",
      "8             13...)\n",
      "one hot finished\n",
      "cv finished\n",
      "______________________________________________________transform - 250.5s, 4.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangsun/.local/lib/python3.6/site-packages/ipykernel_launcher.py:11: UserWarning: Persisting input arguments took 25.15s to run.\n",
      "If this happens often in your code, it can cause performance problems \n",
      "(results will be correct in all cases). \n",
      "The reason for this is probably some large input arguments for a wrapped\n",
      " function (e.g. large strings).\n",
      "THIS IS A JOBLIB ISSUE. If you can, kindly provide the joblib's team with an\n",
      " example so that they can fix the problem.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    }
   ],
   "source": [
    "fe = FeatureExtractor()\n",
    "X_train_sparse = fe.transform(X_train)\n",
    "X_test_sparse = fe.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Classifier()\n",
    "clf.fit(X_train_sparse, y_train)\n",
    "y_train_pred = clf.predict_proba(X_train_sparse)\n",
    "y_test_pred = clf.predict_proba(X_test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    642581\n",
       "0    281294\n",
       "dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(y_train_pred, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    275676\n",
       "0    120271\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(y_test_pred, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "model = make_pipeline(FeatureExtractor(), Classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one hot finished\n",
      "cv finished\n",
      "one hot finished\n",
      "cv finished\n",
      "one hot finished\n",
      "cv finished\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_train_pred = model.predict_proba(X_train)\n",
    "y_test_pred = model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    642581\n",
       "0    281294\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(y_train_pred, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    275676\n",
       "0    120271\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(np.argmax(y_test_pred, axis=1)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
